{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Open Source LLM for Customer Service Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krupc\\Downloads\\Projects\\mlops\\falcon-chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, pipeline\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset, Features, ClassLabel, Value, Sequence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "GPU Model:  NVIDIA GeForce RTX 4060 Ti\n",
      "Total Memory [GB] of GPU:  8.6\n"
     ]
    }
   ],
   "source": [
    "# Checking the GPU\n",
    "if torch.cuda.is_available():\n",
    "    print('Number of GPUs:', torch.cuda.device_count())\n",
    "    print('GPU Model: ', torch.cuda.get_device_name(0))\n",
    "    print('Total Memory [GB] of GPU: ', round(torch.cuda.get_device_properties(0).total_memory / 1e9, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Quantization Parameters\n",
    "Simply put, these parameters are used to make the model fit into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`load_in_4bit=True`**\n",
    "   - This parameter enables loading the model weights in **4-bit precision**. Quantization to 4-bit reduces the memory footprint of the model significantly, making it faster and more efficient for inference, especially in environments with limited GPU resources.\n",
    "\n",
    "2. **`bnb_4bit_compute_dtype=torch.float16`**\n",
    "   - Specifies the data type used for computations during 4-bit quantized operations. \n",
    "   - In this case, **`torch.float16`** (16-bit floating-point) is chosen. Using `float16` helps balance precision and performance, as it provides higher precision compared to integer types while still being efficient.\n",
    "\n",
    "3. **`bnb_4bit_quant_type='nf4'`**\n",
    "   - Defines the type of quantization to apply. **`nf4`** stands for \"Normalized Floating-point 4-bit.\"\n",
    "   - **`nf4`** is a specialized quantization format designed for neural network activations and weights. It offers better accuracy compared to traditional quantization techniques like `fp4` or `int4` because it normalizes the range of values, preserving more information.\n",
    "\n",
    "4. **`bnb_4bit_use_double_quant=True`**\n",
    "   - Enables **double quantization** for 4-bit weights.\n",
    "   - Double quantization involves first quantizing the model weights into 4-bit values and then further compressing these values for storage and transfer. This can improve efficiency and reduce memory usage during training or inference.\n",
    "\n",
    "5. **`llm_int8_enable_fp32_cpu_offload=True`**\n",
    "   - Allows offloading specific computations to the CPU with **FP32 precision** (32-bit floating-point).\n",
    "   - This is particularly useful for large language models (LLMs) when the GPU cannot handle certain tasks due to memory constraints. \n",
    "   - With this enabled, operations that require higher precision can offload to the CPU, ensuring no loss in accuracy while optimizing GPU memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the paramaters\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16,\n",
    "                                         bnb_4bit_quant_type='nf4',\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         llm_int8_enable_fp32_cpu_offload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load the Model and the Tokenizer\n",
    "\n",
    "https://huggingface.co/tiiuae/falcon-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.38s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FalconForCausalLM(\n",
      "  (transformer): FalconModel(\n",
      "    (word_embeddings): Embedding(65024, 4544)\n",
      "    (h): ModuleList(\n",
      "      (0-31): 32 x FalconDecoderLayer(\n",
      "        (self_attention): FalconAttention(\n",
      "          (query_key_value): Linear4bit(in_features=4544, out_features=4672, bias=False)\n",
      "          (dense): Linear4bit(in_features=4544, out_features=4544, bias=False)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (rotary_emb): FalconRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): FalconMLP(\n",
      "          (dense_h_to_4h): Linear4bit(in_features=4544, out_features=18176, bias=False)\n",
      "          (act): GELUActivation()\n",
      "          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544, bias=False)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): FalconRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = AutoModelForCausalLM.from_pretrained('tiiuae/falcon-7b',\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             device_map='auto')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='tiiuae/falcon-7b', vocab_size=65024, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'additional_special_tokens': ['>>TITLE<<', '>>ABSTRACT<<', '>>INTRODUCTION<<', '>>SUMMARY<<', '>>COMMENT<<', '>>ANSWER<<', '>>QUESTION<<', '>>DOMAIN<<', '>>PREFIX<<', '>>SUFFIX<<', '>>MIDDLE<<']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\">>TITLE<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\">>ABSTRACT<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\">>INTRODUCTION<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\">>SUMMARY<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\">>COMMENT<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\">>ANSWER<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\">>QUESTION<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\">>DOMAIN<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\">>PREFIX<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\">>SUFFIX<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\">>MIDDLE<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('tiiuae/falcon-7b')\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing the Original Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`param.requires_grad = False`**: This disables gradient computation for each parameter, which means these parameters will not be updated during training. This is commonly done when freezing certain parts of a pre-trained model for transfer learning, where only the top layers of the model are trained.\n",
    "\n",
    "**`if param.ndim == 1: param.data = param.data.to(torch.float32)`**: This line converts one-dimensional parameters (i.e., vectors) to the `float32` data type. This might be necessary to ensure that all model parameters are of the same data type, especially when performing operations that require consistent data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    if param.ndim == 1:\n",
    "        param.data = param.data.to(torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling the Checkpoint of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables the gradient checkpoint feature in the model\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When gradient checkpointing is enabled, the model does not store all intermediate values (layer activations) during the forward pass. Instead, it saves only a few checkpointed values. During the backward pass, the intermediate values that were not stored are recomputed from the checkpoints. This reduces the amount of memory required to store intermediate values but increases computation time because some values need to be recomputed.\n",
    "\n",
    "This technique is useful when training very large models that would otherwise not fit into GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables the technique called \"gradient checkpointing\"\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is useful for reducing memory consumption during the training of large models. It works by saving certain intermediate states (checkpoints) during the forward pass and then using these states during the backward pass to recompute the gradients instead of storing all intermediate states in memory. This can reduce the amount of memory required but may increase computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting Conversion to Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CastOutputToFloat class is a subclass of PyTorch's nn.Sequential class, which is used to create a sequence of modules (like layers of a neural network). The main functionality of this class is to convert the data type of the output of a sequence of modules to torch.float32 (i.e., a 32-bit floating-point tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Conversion\n",
    "class CastOutputFloat(nn.Sequential):\n",
    "    def forward(self, input):\n",
    "        return super().forward(input).to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is replacing the language model head (lm_head) with the CastOutputToFloat module that wraps the original language model head. This means that every time the model's lm_head is used during the forward pass, its output will automatically be converted to torch.float32. This can be useful for ensuring type compatibility in situations where the output of the language model head needs to be of type float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head = CastOutputFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Fine Tuning Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LoRA**\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a technique designed to make fine-tuning large-scale machine learning models more efficient and computationally feasible. It achieves this by introducing low-rank matrices into the model, allowing for parameter-efficient fine-tuning while maintaining performance comparable to traditional methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me break down the parameters in the `LoraConfig` configuration:\n",
    "\n",
    "1. **`r = 16`**\n",
    "   - This specifies the **rank** of the low-rank adaptation (LoRA) matrices.\n",
    "   - LoRA replaces parts of the weight matrices in a neural network with low-rank matrices to reduce the number of trainable parameters.\n",
    "   - A higher rank (`r`) increases the model's capacity but also increases the number of trainable parameters.\n",
    "\n",
    "2. **`lora_alpha = 32`**\n",
    "   - This is a scaling factor used in the LoRA mechanism.\n",
    "   - The output of the LoRA weights is scaled by `lora_alpha / r`.\n",
    "   - It controls the contribution of the LoRA layers to the final output, allowing fine-tuning of the impact of the low-rank updates.\n",
    "\n",
    "3. **`lora_dropout = 0.05`**\n",
    "   - Specifies the dropout rate for the LoRA layers.\n",
    "   - Dropout randomly zeroes out some connections in the LoRA layers during training to prevent overfitting.\n",
    "   - A value of `0.05` means there is a 5% chance of a connection being dropped during training.\n",
    "\n",
    "4. **`bias = \"none\"`**\n",
    "   - Indicates whether biases in the model should be adapted.\n",
    "   - Options include:\n",
    "     - `\"none\"`: No biases are adapted.\n",
    "     - `\"all\"`: All biases in the model are adapted.\n",
    "     - `\"lora_only\"`: Only biases in LoRA layers are adapted.\n",
    "\n",
    "5. **`task_type = \"CAUSAL_LM\"`**\n",
    "   - Specifies the type of task the model is being fine-tuned for.\n",
    "   - In this case, `\"CAUSAL_LM\"` stands for **Causal Language Modeling**, which involves predicting the next token in a sequence (as in autoregressive models like GPT).\n",
    "   - Other possible task types might include `\"SEQ2SEQ_LM\"` (sequence-to-sequence language modeling) or `\"TOKEN_CLASSIFICATION\"`, depending on the specific LoRA implementation.\n",
    "\n",
    "Summary\n",
    "This `LoraConfig` is designed to apply LoRA to a model in a way that reduces the number of trainable parameters, making fine-tuning more efficient. It's specifically configured for causal language modeling tasks, with a moderate rank (`r=16`), a scaling factor (`lora_alpha=32`), and a small amount of dropout (`lora_dropout=0.05`) to balance between regularization and performance. The bias is not adapted in this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRa Config\n",
    "config = LoraConfig(r = 16,\n",
    "                    lora_alpha = 32,\n",
    "                    lora_dropout = 0.05,\n",
    "                    bias = \"none\",\n",
    "                    task_type = \"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the model considering the LoRA parameters\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to print the trainable parameters of a model\n",
    "def print_trainable_parameters(model):\n",
    "\n",
    "    # Initialize the count of trainable parameters\n",
    "    trainable_params = 0\n",
    "\n",
    "    # Initialize the count of all parameters\n",
    "    all_param = 0\n",
    "\n",
    "    # Iterate over all named parameters of the model\n",
    "    for _, param in model.named_parameters():\n",
    "\n",
    "        # Sum the total number of elements of all parameters\n",
    "        all_param += param.numel()\n",
    "\n",
    "        # Check if the parameter is trainable\n",
    "        if param.requires_grad:\n",
    "\n",
    "            # Sum the number of elements of the trainable parameters\n",
    "            trainable_params += param.numel()\n",
    "\n",
    "    # Print the number of trainable parameters, the total number of parameters, and the percentage of trainable parameters\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4718592 || all params: 5477012352 || trainable%: 0.08615266310796153\n"
     ]
    }
   ],
   "source": [
    "# Execute the function\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
