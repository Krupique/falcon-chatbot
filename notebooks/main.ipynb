{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Open Source LLM for Customer Service Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krupc\\Downloads\\Projects\\mlops\\falcon-chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, pipeline\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset, Features, ClassLabel, Value, Sequence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "GPU Model:  NVIDIA GeForce RTX 4060 Ti\n",
      "Total Memory [GB] of GPU:  8.6\n"
     ]
    }
   ],
   "source": [
    "# Checking the GPU\n",
    "if torch.cuda.is_available():\n",
    "    print('Number of GPUs:', torch.cuda.device_count())\n",
    "    print('GPU Model: ', torch.cuda.get_device_name(0))\n",
    "    print('Total Memory [GB] of GPU: ', round(torch.cuda.get_device_properties(0).total_memory / 1e9, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Quantization Parameters\n",
    "Simply put, these parameters are used to make the model fit into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`load_in_4bit=True`**\n",
    "   - This parameter enables loading the model weights in **4-bit precision**. Quantization to 4-bit reduces the memory footprint of the model significantly, making it faster and more efficient for inference, especially in environments with limited GPU resources.\n",
    "\n",
    "2. **`bnb_4bit_compute_dtype=torch.float16`**\n",
    "   - Specifies the data type used for computations during 4-bit quantized operations. \n",
    "   - In this case, **`torch.float16`** (16-bit floating-point) is chosen. Using `float16` helps balance precision and performance, as it provides higher precision compared to integer types while still being efficient.\n",
    "\n",
    "3. **`bnb_4bit_quant_type='nf4'`**\n",
    "   - Defines the type of quantization to apply. **`nf4`** stands for \"Normalized Floating-point 4-bit.\"\n",
    "   - **`nf4`** is a specialized quantization format designed for neural network activations and weights. It offers better accuracy compared to traditional quantization techniques like `fp4` or `int4` because it normalizes the range of values, preserving more information.\n",
    "\n",
    "4. **`bnb_4bit_use_double_quant=True`**\n",
    "   - Enables **double quantization** for 4-bit weights.\n",
    "   - Double quantization involves first quantizing the model weights into 4-bit values and then further compressing these values for storage and transfer. This can improve efficiency and reduce memory usage during training or inference.\n",
    "\n",
    "5. **`llm_int8_enable_fp32_cpu_offload=True`**\n",
    "   - Allows offloading specific computations to the CPU with **FP32 precision** (32-bit floating-point).\n",
    "   - This is particularly useful for large language models (LLMs) when the GPU cannot handle certain tasks due to memory constraints. \n",
    "   - With this enabled, operations that require higher precision can offload to the CPU, ensuring no loss in accuracy while optimizing GPU memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the paramaters\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16,\n",
    "                                         bnb_4bit_quant_type='nf4',\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         llm_int8_enable_fp32_cpu_offload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load the Model and the Tokenizer\n",
    "\n",
    "https://huggingface.co/tiiuae/falcon-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.38s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FalconForCausalLM(\n",
      "  (transformer): FalconModel(\n",
      "    (word_embeddings): Embedding(65024, 4544)\n",
      "    (h): ModuleList(\n",
      "      (0-31): 32 x FalconDecoderLayer(\n",
      "        (self_attention): FalconAttention(\n",
      "          (query_key_value): Linear4bit(in_features=4544, out_features=4672, bias=False)\n",
      "          (dense): Linear4bit(in_features=4544, out_features=4544, bias=False)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (rotary_emb): FalconRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): FalconMLP(\n",
      "          (dense_h_to_4h): Linear4bit(in_features=4544, out_features=18176, bias=False)\n",
      "          (act): GELUActivation()\n",
      "          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544, bias=False)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): FalconRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = AutoModelForCausalLM.from_pretrained('tiiuae/falcon-7b',\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             device_map='auto')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='tiiuae/falcon-7b', vocab_size=65024, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'additional_special_tokens': ['>>TITLE<<', '>>ABSTRACT<<', '>>INTRODUCTION<<', '>>SUMMARY<<', '>>COMMENT<<', '>>ANSWER<<', '>>QUESTION<<', '>>DOMAIN<<', '>>PREFIX<<', '>>SUFFIX<<', '>>MIDDLE<<']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\">>TITLE<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\">>ABSTRACT<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\">>INTRODUCTION<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\">>SUMMARY<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\">>COMMENT<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\">>ANSWER<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\">>QUESTION<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\">>DOMAIN<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\">>PREFIX<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\">>SUFFIX<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\">>MIDDLE<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('tiiuae/falcon-7b')\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing the Original Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`param.requires_grad = False`**: This disables gradient computation for each parameter, which means these parameters will not be updated during training. This is commonly done when freezing certain parts of a pre-trained model for transfer learning, where only the top layers of the model are trained.\n",
    "\n",
    "**`if param.ndim == 1: param.data = param.data.to(torch.float32)`**: This line converts one-dimensional parameters (i.e., vectors) to the `float32` data type. This might be necessary to ensure that all model parameters are of the same data type, especially when performing operations that require consistent data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    if param.ndim == 1:\n",
    "        param.data = param.data.to(torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling the Checkpoint of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables the gradient checkpoint feature in the model\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When gradient checkpointing is enabled, the model does not store all intermediate values (layer activations) during the forward pass. Instead, it saves only a few checkpointed values. During the backward pass, the intermediate values that were not stored are recomputed from the checkpoints. This reduces the amount of memory required to store intermediate values but increases computation time because some values need to be recomputed.\n",
    "\n",
    "This technique is useful when training very large models that would otherwise not fit into GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables the technique called \"gradient checkpointing\"\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is useful for reducing memory consumption during the training of large models. It works by saving certain intermediate states (checkpoints) during the forward pass and then using these states during the backward pass to recompute the gradients instead of storing all intermediate states in memory. This can reduce the amount of memory required but may increase computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting Conversion to Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CastOutputToFloat class is a subclass of PyTorch's nn.Sequential class, which is used to create a sequence of modules (like layers of a neural network). The main functionality of this class is to convert the data type of the output of a sequence of modules to torch.float32 (i.e., a 32-bit floating-point tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Conversion\n",
    "class CastOutputFloat(nn.Sequential):\n",
    "    def forward(self, input):\n",
    "        return super().forward(input).to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is replacing the language model head (lm_head) with the CastOutputToFloat module that wraps the original language model head. This means that every time the model's lm_head is used during the forward pass, its output will automatically be converted to torch.float32. This can be useful for ensuring type compatibility in situations where the output of the language model head needs to be of type float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head = CastOutputFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Fine Tuning Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LoRA**\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a technique designed to make fine-tuning large-scale machine learning models more efficient and computationally feasible. It achieves this by introducing low-rank matrices into the model, allowing for parameter-efficient fine-tuning while maintaining performance comparable to traditional methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me break down the parameters in the `LoraConfig` configuration:\n",
    "\n",
    "1. **`r = 16`**\n",
    "   - This specifies the **rank** of the low-rank adaptation (LoRA) matrices.\n",
    "   - LoRA replaces parts of the weight matrices in a neural network with low-rank matrices to reduce the number of trainable parameters.\n",
    "   - A higher rank (`r`) increases the model's capacity but also increases the number of trainable parameters.\n",
    "\n",
    "2. **`lora_alpha = 32`**\n",
    "   - This is a scaling factor used in the LoRA mechanism.\n",
    "   - The output of the LoRA weights is scaled by `lora_alpha / r`.\n",
    "   - It controls the contribution of the LoRA layers to the final output, allowing fine-tuning of the impact of the low-rank updates.\n",
    "\n",
    "3. **`lora_dropout = 0.05`**\n",
    "   - Specifies the dropout rate for the LoRA layers.\n",
    "   - Dropout randomly zeroes out some connections in the LoRA layers during training to prevent overfitting.\n",
    "   - A value of `0.05` means there is a 5% chance of a connection being dropped during training.\n",
    "\n",
    "4. **`bias = \"none\"`**\n",
    "   - Indicates whether biases in the model should be adapted.\n",
    "   - Options include:\n",
    "     - `\"none\"`: No biases are adapted.\n",
    "     - `\"all\"`: All biases in the model are adapted.\n",
    "     - `\"lora_only\"`: Only biases in LoRA layers are adapted.\n",
    "\n",
    "5. **`task_type = \"CAUSAL_LM\"`**\n",
    "   - Specifies the type of task the model is being fine-tuned for.\n",
    "   - In this case, `\"CAUSAL_LM\"` stands for **Causal Language Modeling**, which involves predicting the next token in a sequence (as in autoregressive models like GPT).\n",
    "   - Other possible task types might include `\"SEQ2SEQ_LM\"` (sequence-to-sequence language modeling) or `\"TOKEN_CLASSIFICATION\"`, depending on the specific LoRA implementation.\n",
    "\n",
    "Summary\n",
    "This `LoraConfig` is designed to apply LoRA to a model in a way that reduces the number of trainable parameters, making fine-tuning more efficient. It's specifically configured for causal language modeling tasks, with a moderate rank (`r=16`), a scaling factor (`lora_alpha=32`), and a small amount of dropout (`lora_dropout=0.05`) to balance between regularization and performance. The bias is not adapted in this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRa Config\n",
    "config = LoraConfig(r = 16,\n",
    "                    lora_alpha = 32,\n",
    "                    lora_dropout = 0.05,\n",
    "                    bias = \"none\",\n",
    "                    task_type = \"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the model considering the LoRA parameters\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to print the trainable parameters of a model\n",
    "def print_trainable_parameters(model):\n",
    "\n",
    "    # Initialize the count of trainable parameters\n",
    "    trainable_params = 0\n",
    "\n",
    "    # Initialize the count of all parameters\n",
    "    all_param = 0\n",
    "\n",
    "    # Iterate over all named parameters of the model\n",
    "    for _, param in model.named_parameters():\n",
    "\n",
    "        # Sum the total number of elements of all parameters\n",
    "        all_param += param.numel()\n",
    "\n",
    "        # Check if the parameter is trainable\n",
    "        if param.requires_grad:\n",
    "\n",
    "            # Sum the number of elements of the trainable parameters\n",
    "            trainable_params += param.numel()\n",
    "\n",
    "    # Print the number of trainable parameters, the total number of parameters, and the percentage of trainable parameters\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4718592 || all params: 5477012352 || trainable%: 0.08615266310796153\n"
     ]
    }
   ],
   "source": [
    "# Execute the function\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questions': [{'question': 'I was charged incorrectly. What should I do?',\n",
       "   'answer': \"If Booking.com charged you incorrectly, visit the Help Center. We'll likely need a copy of your bank statement showing the incorrect charges as well as your booking confirmation number and PIN. If the property charged you incorrectly, reach out to the property directly for help.\"},\n",
       "  {'question': 'How do I confirm that I paid for my booking?',\n",
       "   'answer': \"Youâ€™ll find payment confirmation in your confirmation email. In the email, there's also an option to download the payment confirmation as a .pdf.\"},\n",
       "  {'question': \"Where can I find my booking's payment policy?\",\n",
       "   'answer': \"Booking.com will charge your card for the total price of the booking when you book. If you're looking for more info about your payment, check your confirmation email.\"},\n",
       "  {'question': 'I was charged. Do I need to do anything else?',\n",
       "   'answer': \"No, you're all set. We charged you for the price of the booking. If there are additional taxes or charges, this info should be displayed on the property page.\"},\n",
       "  {'question': 'Will I need to pay anything extra on arrival?',\n",
       "   'answer': \"Sometimes you're expected to pay taxes and charges upon arrival. If this is the case, the amount is stated before you book and in your confirmation email. If no amount is stated, you shouldn't pay anything upon arrival. If the property requests money that you believe is incorrect, go to the Help Center for further support.\"},\n",
       "  {'question': 'How do I add my booking to my account?',\n",
       "   'answer': \"Unfortunately, it isn't possible to add this booking to your account. We're working on a way to enable this in the future.\"},\n",
       "  {'question': 'Are meals included in my booking?',\n",
       "   'answer': \"Your confirmation email will list any meals that are or aren't included. Depending on the property, you might be able to add a meal plan on arrival. Contact the property directly 1 or 2 days before your stay to learn more.\"},\n",
       "  {'question': \"On arrival, the room/accommodation I was given didn't match what I booked. What should I do?\",\n",
       "   'answer': \"Visit the Help Center. You'll likely need to send our Customer Service team a screenshot of the mismatched info andâ€”if you had to pay any extra feesâ€”a receipt as proof of payment.\"},\n",
       "  {'question': 'My booking was canceled. When will I be refunded? ',\n",
       "   'answer': 'If your booking gets canceled, Booking.com refunds you immediately. The processing time takes 7 to 12 days, depending on your bank. For any questions, contact your bank directly.'},\n",
       "  {'question': 'Can I cancel my booking?',\n",
       "   'answer': \"If your booking is non-refundable, you can't cancel. If your booking is free cancellation or partially refundable, check your confirmation email or the confirmation page under the 'Bookings' section of your account for the cancellation fees.\"},\n",
       "  {'question': 'Can I make changes to my booking?',\n",
       "   'answer': \"It's not possible to make any changes to your booking. This includes actions like changing the stay dates, guest name, or email address.\"},\n",
       "  {'question': 'My booking was canceled or declined â€“ what should I do?',\n",
       "   'answer': \"Because this booking is facilitated by a partner company, the property may be overbooked. If that's the case, the partner will notify us. Next, we'll refund and cancel your booking. The processing time can take 7 to 10 days and depends on your bank. If you have questions, contact your bank directly.\"},\n",
       "  {'question': 'If I need to cancel my booking, will I pay a fee?',\n",
       "   'answer': \"If your booking is non-refundable, you can't cancel or make changes. If your booking is free cancellation or partially refundable, check your confirmation email or the confirmation page under the 'Bookings' section of your account for the cancellation fees.\"},\n",
       "  {'question': 'How do I know if my booking was canceled?',\n",
       "   'answer': 'After canceling a booking with us, you should get an email confirming the cancellation. Check your inbox and spam/junk mail folders. If you get an email from Booking.com about canceling your booking, your booking was canceled and we refunded you. The processing time will depend on your bank. For any questions, contact your bank directly.'},\n",
       "  {'question': 'Where can I find the cancellation policy?',\n",
       "   'answer': \"You'll find it in your booking confirmation email.\"},\n",
       "  {'question': 'Can I use a coupon or promotional link to make this booking?',\n",
       "   'answer': \"No, this booking can't be made with rewards or incentive programs. Even if you use a unique link or code to make this booking, you won't be rewarded for this stay.\"},\n",
       "  {'question': 'Can I use this booking towards my Genius level or membership?',\n",
       "   'answer': \"No, this booking doesn't count towards your Genius bookings or membership level.\"},\n",
       "  {'question': 'Is this booking eligible for We Price Match?',\n",
       "   'answer': \"No, this booking isn't eligible for We Price Match.\"},\n",
       "  {'question': 'Can I use credit to make this booking?',\n",
       "   'answer': 'No, itâ€™s not possible to use travel credit toward bookings facilitated by partner travel companies. Choose a different room or rate instead.'},\n",
       "  {'question': 'I paid for my booking but havenâ€™t received a confirmation email. What should I do?',\n",
       "   'answer': \"You'll get a confirmation email once your booking is confirmed. This should only take about two minutes but sometimes can take longer. Check your spam/junk mail folders for this email. It may be there by mistake. If you still haven't received any emails, reach out to our Customer Service team.\"},\n",
       "  {'question': 'Can you resend my confirmation email?',\n",
       "   'answer': 'We always resend your confirmation email a few days before check-in. If you have a Booking.com account, you can also find your confirmation under the Bookings section of your account. If you still need us to resend your confirmation email, reach out to our Customer Service team.'},\n",
       "  {'question': \"I can't find my confirmation email. What should I do?\",\n",
       "   'answer': 'First check your inbox and spam/junk mail folders â€“ it might end up there by mistake. If you have a Booking.com account, you can also find your confirmation under the Bookings section of your account.'},\n",
       "  {'question': 'Why and how long do I need to wait for my booking to be confirmed?',\n",
       "   'answer': \"We'll need to check with our partner travel company to make sure your selection is still available. We do our best to let you know within two minutes by email. Check your spam/junk mail folders in case this email ended up there by mistake.\"},\n",
       "  {'question': \"My booking still isn't confirmed. What should I do?\",\n",
       "   'answer': \"Youâ€™ll get an email to let you know if your booking is confirmed within several minutes. Check your inbox and spam/junk mail folders for this email. If you still can't find the email, reach out to our Customer Service team.\"},\n",
       "  {'question': \"My confirmation number doesn't work when I call Customer Service â€“ what should I do?\",\n",
       "   'answer': \"If you contacted us by phone, the confirmation number may be too long to register correctly. Instead, go to the Help Center. If you're already speaking with a Customer Service agent, let them know this is an offer facilitated by a partner company (i.e. a 'Partner offer').\"},\n",
       "  {'question': \"The property is double-booked and/or can't find my booking. What should I do?\",\n",
       "   'answer': 'Visit the Help Center as soon as possible.'},\n",
       "  {'question': 'How do I request extra services from the property?',\n",
       "   'answer': \"To request an extra service like an extra bed, early check-in, or transportation to the property, contact the property directly 1 or 2 days before arrival. There's no guarantee, but they might be able to help.\"},\n",
       "  {'question': 'How do I request early or late check-in/-out?',\n",
       "   'answer': \"To request a different check-in/-out time, contact the property directly 1 or 2 days before arrival. There's no guarantee, but they might be able to help.\"},\n",
       "  {'question': 'How do I get more info about a propertyâ€™s facilities?',\n",
       "   'answer': \"Click the property name on the confirmation page to visit the property page. You'll see a complete list of property facilities there.\"},\n",
       "  {'question': 'Can I change the guest name for this booking?',\n",
       "   'answer': \"Itâ€™s not possible to change any personal details like the guest's name or email address.\"},\n",
       "  {'question': 'Can I get extra beds/cribs for children?',\n",
       "   'answer': \"It depends on the propertyâ€™s policy. Typically, additional costs for children (including extra beds/cribs) aren't included in the price. Contact the property directly 1â€“2 days before your stay to find out more.\"},\n",
       "  {'question': \"I can't find my booking in my account. What should I do?\",\n",
       "   'answer': \"If you were signed in when booking, it should appear in the Bookings section of your account. If you weren't signed in when booking, the booking won't appear, and you can't add this booking to your account. If you donâ€™t have an account or weren't signed in when booking, check your inbox for your confirmation email.\"},\n",
       "  {'question': 'How do I add my booking to my account?',\n",
       "   'answer': \"Unfortunately, it isn't possible to add this booking to your account. We're working on a way to enable this in the future.\"},\n",
       "  {'question': 'Are meals included in my booking?',\n",
       "   'answer': \"Your confirmation email will list any meals that are or aren't included. Depending on the property, you might be able to add a meal plan on arrival. Contact the property directly 1 or 2 days before your stay to learn more.\"},\n",
       "  {'question': \"On arrival, the room/accommodation I was given didn't match what I booked. What should I do?\",\n",
       "   'answer': \"Visit the Help Center. You'll likely need to send our Customer Service team a screenshot of the mismatched info andâ€”if you had to pay any extra feesâ€”a receipt as proof of payment.\"},\n",
       "  {'question': 'What is a Partner offer?',\n",
       "   'answer': \"To offer you more competitive prices, we sometimes partner with other companies. These offers are always paid for in advance and can't be booked in combination with other offers. Additionally, any changes to your personal or booking details after booking aren't possible.\"},\n",
       "  {'question': 'How do you keep my personal details safe when working with partner companies?',\n",
       "   'answer': 'We only provide partner companies (Trip Providers) with the info necessary to complete your booking. Read our Privacy statement for full details.'}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the file\n",
    "file = open(\"../data/frequently_aksed_questions.json\")\n",
    "\n",
    "# Load the file\n",
    "data = json.load(file)\n",
    "\n",
    "# Printing the data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was charged incorrectly. What should I do?\n"
     ]
    }
   ],
   "source": [
    "# Questions and Answers Lists\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "# Loor to extract the questions and answers\n",
    "for i in data['questions']:\n",
    "    questions+=[i['question']]\n",
    "    answers+=[i['answer']]\n",
    "\n",
    "\n",
    "print(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put the data in the correct format to train the model\n",
    "dataset = Dataset.from_dict({\n",
    "    \"id\": list(range(len(questions))),\n",
    "    \"questions\": questions,\n",
    "    \"answers\": answers\n",
    "    },\n",
    "    features= Features({\n",
    "        \"id\": Value(dtype=\"string\"),\n",
    "        \"questions\": Value(dtype=\"string\"),\n",
    "        \"answers\": Value(dtype=\"string\")\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test\n",
    "dataset = dataset.train_test_split(test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge columns by concatenating each question with the corresponding answer\n",
    "def merge_columns(reg):\n",
    "    reg['output'] = reg['questions'] + \" ->: \" + reg['answers']\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 31/31 [00:00<00:00, 2296.75 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 2999.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Applying the function\n",
    "dataset = dataset.map(merge_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How do I request extra services from the property? ->: To request an extra service like an extra bed, early check-in, or transportation to the property, contact the property directly 1 or 2 days before arrival. There's no guarantee, but they might be able to help.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will be the data format to train our model\n",
    "dataset['train']['output'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '26',\n",
       " 'questions': 'How do I request extra services from the property?',\n",
       " 'answers': \"To request an extra service like an extra bed, early check-in, or transportation to the property, contact the property directly 1 or 2 days before arrival. There's no guarantee, but they might be able to help.\",\n",
       " 'output': \"How do I request extra services from the property? ->: To request an extra service like an extra bed, early check-in, or transportation to the property, contact the property directly 1 or 2 days before arrival. There's no guarantee, but they might be able to help.\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that we have an Id, an input (questions and answers) and we have an output (questions and answers combination)\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 31/31 [00:00<00:00, 620.86 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 1499.04 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '26',\n",
       " 'questions': 'How do I request extra services from the property?',\n",
       " 'answers': \"To request an extra service like an extra bed, early check-in, or transportation to the property, contact the property directly 1 or 2 days before arrival. There's no guarantee, but they might be able to help.\",\n",
       " 'output': \"How do I request extra services from the property? ->: To request an extra service like an extra bed, early check-in, or transportation to the property, contact the property directly 1 or 2 days before arrival. There's no guarantee, but they might be able to help.\",\n",
       " 'input_ids': [1830,\n",
       "  441,\n",
       "  295,\n",
       "  2726,\n",
       "  2535,\n",
       "  1626,\n",
       "  427,\n",
       "  248,\n",
       "  2891,\n",
       "  42,\n",
       "  204,\n",
       "  1579,\n",
       "  37,\n",
       "  1472,\n",
       "  2726,\n",
       "  267,\n",
       "  2535,\n",
       "  1506,\n",
       "  606,\n",
       "  267,\n",
       "  2535,\n",
       "  2328,\n",
       "  23,\n",
       "  2304,\n",
       "  1683,\n",
       "  24,\n",
       "  242,\n",
       "  23,\n",
       "  379,\n",
       "  9386,\n",
       "  271,\n",
       "  248,\n",
       "  2891,\n",
       "  23,\n",
       "  2072,\n",
       "  248,\n",
       "  2891,\n",
       "  3644,\n",
       "  204,\n",
       "  28,\n",
       "  379,\n",
       "  204,\n",
       "  29,\n",
       "  1522,\n",
       "  996,\n",
       "  11651,\n",
       "  25,\n",
       "  1293,\n",
       "  18,\n",
       "  94,\n",
       "  658,\n",
       "  7462,\n",
       "  23,\n",
       "  480,\n",
       "  506,\n",
       "  1295,\n",
       "  314,\n",
       "  1469,\n",
       "  271,\n",
       "  733,\n",
       "  25],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing the data\n",
    "dataset = dataset.map(lambda samples: tokenizer(samples['output']), batched=True)\n",
    "\n",
    "# Tokenized data\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Arguments of Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it doesn't have pad, we adjust the pad of the tokenizer\n",
    "if tokenizer.pad_token == None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Trainer Parameters**\n",
    "\n",
    "**1. `model=model`**\n",
    "   - This is the model to be trained. It is typically a pre-trained model from the `transformers` library, such as BERT, GPT, or T5.\n",
    "\n",
    "**2. `train_dataset=dataset['train']`**\n",
    "   - The training dataset. \n",
    "   - It should be a compatible dataset object, such as one created using the Hugging Face `datasets` library, containing the input data for training.\n",
    "\n",
    "**3. `eval_dataset=dataset['test']`**\n",
    "   - The evaluation dataset. \n",
    "   - This dataset is used to evaluate the model's performance at regular intervals during training (based on the evaluation strategy defined in the training arguments).\n",
    "\n",
    "**4. `args=transformers.TrainingArguments(...)`**\n",
    "   - Contains hyperparameters and settings for training, such as batch size, number of epochs, learning rate, logging steps, etc. (Explained in detail below.)\n",
    "\n",
    "**5. `data_collator=transformers.DataCollatorForLanguageModeling(...)`**\n",
    "   - A data collator is responsible for batching and processing input data.\n",
    "   - **`DataCollatorForLanguageModeling`** is used for language modeling tasks. It prepares batches by padding sequences and can optionally mask tokens for tasks like masked language modeling (MLM).\n",
    "   - In this case:\n",
    "     - `tokenizer=tokenizer`: Specifies the tokenizer used to tokenize input sequences.\n",
    "     - `mlm=False`: Indicates that this is a causal language modeling task (e.g., GPT), where the model predicts the next token in a sequence, not masked tokens.\n",
    "\n",
    "---\n",
    "\n",
    "#### **TrainingArguments Parameters**\n",
    "\n",
    "**1. `evaluation_strategy='epoch'`**\n",
    "   - Specifies how often the model is evaluated on the evaluation dataset.\n",
    "   - Options include:\n",
    "     - `'no'`: No evaluation.\n",
    "     - `'steps'`: Evaluate every `eval_steps`.\n",
    "     - `'epoch'`: Evaluate at the end of every epoch.\n",
    "\n",
    "**2. `per_device_train_batch_size=2`**\n",
    "   - Sets the batch size for training per device (e.g., GPU or CPU).\n",
    "   - A smaller batch size reduces memory consumption but may increase training time due to more frequent gradient updates.\n",
    "\n",
    "**3. `gradient_accumulation_steps=2`**\n",
    "   - Specifies how many steps to accumulate gradients before performing a backward pass and optimizer step.\n",
    "   - Effectively increases the batch size without requiring more GPU memory by simulating a larger batch over multiple steps.\n",
    "\n",
    "**4. `num_train_epochs=10`**\n",
    "   - Defines the total number of training epochs (complete passes over the training dataset).\n",
    "\n",
    "**5. `learning_rate=2e-4`**\n",
    "   - Sets the learning rate for the optimizer. A smaller value can lead to slower but more stable convergence, while a larger value might train faster but risk instability.\n",
    "\n",
    "**6. `fp16=True`**\n",
    "   - Enables **mixed precision training**, which uses half-precision (16-bit floating point) to perform calculations and reduces memory consumption.\n",
    "   - This accelerates training on GPUs with Tensor Cores (e.g., NVIDIA Volta, Turing, or Ampere architectures) while maintaining model performance.\n",
    "\n",
    "**7. `logging_steps=1`**\n",
    "   - Defines how often (in terms of training steps) the training logs (e.g., loss, metrics) are written.\n",
    "   - Setting `1` means logging happens after every step.\n",
    "\n",
    "**8. `output_dir='outputs'`**\n",
    "   - Specifies the directory where the model's checkpoints and training logs will be saved.\n",
    "   - This is important for saving intermediate training results and final model checkpoints for later use or evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Arguments\n",
    "trainer = transformers.Trainer(model = model,\n",
    "                               train_dataset= dataset['train'],\n",
    "                               eval_dataset=dataset['test'],\n",
    "                               args=transformers.TrainingArguments(\n",
    "                                   evaluation_strategy='epoch',\n",
    "                                   per_device_train_batch_size=2,\n",
    "                                   gradient_accumulation_steps=2,\n",
    "                                   num_train_epochs=10,\n",
    "                                   learning_rate=2e-4,\n",
    "                                   fp16=True,\n",
    "                                   logging_steps=1,\n",
    "                                   output_dir='outputs'\n",
    "                               ),\n",
    "                               data_collator=transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
