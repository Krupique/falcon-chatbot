{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Open Source LLM for Customer Service Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krupc\\Downloads\\Projects\\mlops\\falcon-chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, pipeline\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset, Features, ClassLabel, Value, Sequence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "GPU Model:  NVIDIA GeForce RTX 4060 Ti\n",
      "Total Memory [GB] of GPU:  8.6\n"
     ]
    }
   ],
   "source": [
    "# Checking the GPU\n",
    "if torch.cuda.is_available():\n",
    "    print('Number of GPUs:', torch.cuda.device_count())\n",
    "    print('GPU Model: ', torch.cuda.get_device_name(0))\n",
    "    print('Total Memory [GB] of GPU: ', round(torch.cuda.get_device_properties(0).total_memory / 1e9, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Quantization Parameters\n",
    "Simply put, these parameters are used to make the model fit into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`load_in_4bit=True`**\n",
    "   - This parameter enables loading the model weights in **4-bit precision**. Quantization to 4-bit reduces the memory footprint of the model significantly, making it faster and more efficient for inference, especially in environments with limited GPU resources.\n",
    "\n",
    "2. **`bnb_4bit_compute_dtype=torch.float16`**\n",
    "   - Specifies the data type used for computations during 4-bit quantized operations. \n",
    "   - In this case, **`torch.float16`** (16-bit floating-point) is chosen. Using `float16` helps balance precision and performance, as it provides higher precision compared to integer types while still being efficient.\n",
    "\n",
    "3. **`bnb_4bit_quant_type='nf4'`**\n",
    "   - Defines the type of quantization to apply. **`nf4`** stands for \"Normalized Floating-point 4-bit.\"\n",
    "   - **`nf4`** is a specialized quantization format designed for neural network activations and weights. It offers better accuracy compared to traditional quantization techniques like `fp4` or `int4` because it normalizes the range of values, preserving more information.\n",
    "\n",
    "4. **`bnb_4bit_use_double_quant=True`**\n",
    "   - Enables **double quantization** for 4-bit weights.\n",
    "   - Double quantization involves first quantizing the model weights into 4-bit values and then further compressing these values for storage and transfer. This can improve efficiency and reduce memory usage during training or inference.\n",
    "\n",
    "5. **`llm_int8_enable_fp32_cpu_offload=True`**\n",
    "   - Allows offloading specific computations to the CPU with **FP32 precision** (32-bit floating-point).\n",
    "   - This is particularly useful for large language models (LLMs) when the GPU cannot handle certain tasks due to memory constraints. \n",
    "   - With this enabled, operations that require higher precision can offload to the CPU, ensuring no loss in accuracy while optimizing GPU memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the paramaters\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16,\n",
    "                                         bnb_4bit_quant_type='nf4',\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         llm_int8_enable_fp32_cpu_offload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load the Model and the Tokenizer\n",
    "\n",
    "https://huggingface.co/tiiuae/falcon-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.38s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FalconForCausalLM(\n",
      "  (transformer): FalconModel(\n",
      "    (word_embeddings): Embedding(65024, 4544)\n",
      "    (h): ModuleList(\n",
      "      (0-31): 32 x FalconDecoderLayer(\n",
      "        (self_attention): FalconAttention(\n",
      "          (query_key_value): Linear4bit(in_features=4544, out_features=4672, bias=False)\n",
      "          (dense): Linear4bit(in_features=4544, out_features=4544, bias=False)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (rotary_emb): FalconRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): FalconMLP(\n",
      "          (dense_h_to_4h): Linear4bit(in_features=4544, out_features=18176, bias=False)\n",
      "          (act): GELUActivation()\n",
      "          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544, bias=False)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): FalconRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = AutoModelForCausalLM.from_pretrained('tiiuae/falcon-7b',\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             device_map='auto')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='tiiuae/falcon-7b', vocab_size=65024, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'additional_special_tokens': ['>>TITLE<<', '>>ABSTRACT<<', '>>INTRODUCTION<<', '>>SUMMARY<<', '>>COMMENT<<', '>>ANSWER<<', '>>QUESTION<<', '>>DOMAIN<<', '>>PREFIX<<', '>>SUFFIX<<', '>>MIDDLE<<']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\">>TITLE<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\">>ABSTRACT<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\">>INTRODUCTION<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\">>SUMMARY<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\">>COMMENT<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\">>ANSWER<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\">>QUESTION<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\">>DOMAIN<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\">>PREFIX<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\">>SUFFIX<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\">>MIDDLE<<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('tiiuae/falcon-7b')\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing the Original Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`param.requires_grad = False`**: This disables gradient computation for each parameter, which means these parameters will not be updated during training. This is commonly done when freezing certain parts of a pre-trained model for transfer learning, where only the top layers of the model are trained.\n",
    "\n",
    "**`if param.ndim == 1: param.data = param.data.to(torch.float32)`**: This line converts one-dimensional parameters (i.e., vectors) to the `float32` data type. This might be necessary to ensure that all model parameters are of the same data type, especially when performing operations that require consistent data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    if param.ndim == 1:\n",
    "        param.data = param.data.to(torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling the Checkpoint of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables the gradient checkpoint feature in the model\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When gradient checkpointing is enabled, the model does not store all intermediate values (layer activations) during the forward pass. Instead, it saves only a few checkpointed values. During the backward pass, the intermediate values that were not stored are recomputed from the checkpoints. This reduces the amount of memory required to store intermediate values but increases computation time because some values need to be recomputed.\n",
    "\n",
    "This technique is useful when training very large models that would otherwise not fit into GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables the technique called \"gradient checkpointing\"\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is useful for reducing memory consumption during the training of large models. It works by saving certain intermediate states (checkpoints) during the forward pass and then using these states during the backward pass to recompute the gradients instead of storing all intermediate states in memory. This can reduce the amount of memory required but may increase computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting Conversion to Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CastOutputToFloat class is a subclass of PyTorch's nn.Sequential class, which is used to create a sequence of modules (like layers of a neural network). The main functionality of this class is to convert the data type of the output of a sequence of modules to torch.float32 (i.e., a 32-bit floating-point tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Conversion\n",
    "class CastOutputFloat(nn.Sequential):\n",
    "    def forward(self, input):\n",
    "        return super().forward(input).to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is replacing the language model head (lm_head) with the CastOutputToFloat module that wraps the original language model head. This means that every time the model's lm_head is used during the forward pass, its output will automatically be converted to torch.float32. This can be useful for ensuring type compatibility in situations where the output of the language model head needs to be of type float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head = CastOutputFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
